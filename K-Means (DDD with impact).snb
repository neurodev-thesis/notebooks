{
  "metadata" : {
    "name" : "K-Means (DDD with impact)",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#Clustering: K-Means\n\nClustering applied to the metadata of the [DDD cohort](https://decipher.sanger.ac.uk/ddd#overview).\nIt is used after features alignment of the elements to cluster (see the related notebook)."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We define the parameters of the process:\n* the path to our HDFS\n* the common path to the input files (similarity matrix S and set of elements U)\n* the path to the file containing the patients' phenotypes\n* the output path to print out results to HDFS\n\n* the number of clusters to create\n* the number of iterations before considering the model has converged\n* the number of runs of the whole algorithm to compare results based on different stochastic initializations\n\n* the minimal number of patients that must be present in a cluster ; if a cluster is composed of less, the outlier(s) will be excluded and the process started again. This condition can considerably increase the computation time.\n* the minimal number of occurrences of a phenotype for it to be displayed in the output"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val hdfs_path = \"hdfs:/user/ndewit/\"\nval input_path = hdfs_path + \"aligned_DDD_cadd_with_impact\"\nval phenotypes_path = hdfs_path + \"datasets/ddd/ddd3_ega-phenotypes.txt\"\nval output_path = hdfs_path + \"DDD_Kmeans_CADD_with_impact_results.txt\"\n\nval numClusters = 5\nval numIterations = 20\nval numRuns = 1\n\nval outliers_limit = 0\nval nb_min_phenotypes = 0",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "hdfs_path: String = hdfs:/user/ndewit/\ninput_path: String = hdfs:/user/ndewit/aligned_DDD_cadd_with_impact\nphenotypes_path: String = hdfs:/user/ndewit/datasets/ddd/ddd3_ega-phenotypes.txt\noutput_path: String = hdfs:/user/ndewit/DDD_Kmeans_CADD_with_impact.txt\nnumClusters: Int = 5\nnumIterations: Int = 20\nnumRuns: Int = 1\noutliers_limit: Int = 0\nnb_min_phenotypes: Int = 0\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "0"
      },
      "output_type" : "execute_result",
      "execution_count" : 19
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Input retrieval"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.linalg.Vectors\nval data = sc.objectFile[(String, org.apache.spark.mllib.linalg.Vector)](input_path)\nval effects = data.mapValues{ x => x.toArray.takeRight(4) }\nvar samples = data.mapValues{ x => org.apache.spark.mllib.linalg.Vectors.dense(x.toArray.slice(0,3))}\nvar nb_samples = samples.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.mllib.linalg.Vectors\ndata: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[129] at objectFile at <console>:66\neffects: org.apache.spark.rdd.RDD[(String, Array[Double])] = MapPartitionsRDD[130] at mapValues at <console>:67\nsamples: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[131] at mapValues at <console>:68\nnb_samples: Long = 3\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "3"
      },
      "output_type" : "execute_result",
      "execution_count" : 20
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Clustering process"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n\nvar check_outliers = Array[String]()\nvar nb_outliers = check_outliers.size\nvar predicted_clusters = sc.parallelize(Array[(String, Int)]())\nvar kmeans_model = new KMeansModel(Array(org.apache.spark.mllib.linalg.Vectors.dense(Array(0.0, 0.0, 0.0))))\n\ndo {\n\n  val values_only = samples.values\n\n  kmeans_model = KMeans.train(values_only, numClusters, numIterations, numRuns)\n\n  predicted_clusters = samples.mapValues{kmeans_model.predict(_)}.persist\n\n  check_outliers = predicted_clusters.\n  map{ case (patient, cluster) => (cluster, patient) }.\n  aggregateByKey(scala.collection.mutable.HashSet.empty[String])(_+_, _++_).values.\n  flatMap{\n    v =>\n    if (v.size > outliers_limit) { List(\"\") }\n    else { v.toList }\n  }.collect.filter(v => v != \"\")\n  nb_outliers = check_outliers.size\n\n  samples = samples.filter(s => !check_outliers.contains(s._1))\n  nb_samples = samples.count\n\n  println(nb_outliers + \" outliers removed \" +\n          \"(\" + check_outliers.mkString(\", \") + \") \" +\n          \": \" + nb_samples + \" samples remaining.\")\n\n} while (nb_outliers > 0 && nb_samples > numClusters)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "0 outliers removed () : 3 samples remaining.\nimport org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\ncheck_outliers: Array[String] = Array()\nnb_outliers: Int = 0\npredicted_clusters: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[163] at mapValues at <console>:95\nkmeans_model: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@405f965a\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 21
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Clustering evaluation and output of results"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We output our clustering results to a single file on HDFS. The file does not get written out until the *close()* method is reached."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.hadoop.fs._\nimport java.io.BufferedOutputStream\n\nval fs = FileSystem.get(sc.hadoopConfiguration)\n\nclass TextFile(file_path : String) {\n  val physical_file = fs.create(new Path(file_path))\n  val stream = new BufferedOutputStream(physical_file)\n\n  def write(text : String) : Unit = {\n    stream.write(text.getBytes(\"UTF-8\"))\n  }\n\n  def close() : Unit = {\n    stream.close()\n  }\n}\n\nval t = new TextFile(output_path)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.hadoop.fs._\nimport java.io.BufferedOutputStream\nfs: org.apache.hadoop.fs.FileSystem = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1825970379_10, ugi=ndewit (auth:SIMPLE)]]\ndefined class TextFile\nt: TextFile = $iwC$$iwC$TextFile@2afc465\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "$line44.$read$$iwC$$iwC$TextFile@2afc465"
      },
      "output_type" : "execute_result",
      "execution_count" : 22
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val centers = kmeans_model.clusterCenters\n\nvar txt = \"Centroids:\"\n//println(txt)\nt.write(txt + \"\\n\")\n\ncenters.foreach{\n  x =>\n  txt = x.toArray.mkString(\", \")\n  //println(txt)\n  t.write(txt + \"\\n\")\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "centers: Array[org.apache.spark.mllib.linalg.Vector] = Array([0.9570796890133311,1.1169206542542995,1.1503946170861015], [-1.038002847446633,-0.812165252335759,-0.6614769048245084], [0.08092315843328314,-0.304755401918555,-0.48891771226159314], [0.9570796890133311,1.1169206542542995,1.1503946170861015], [0.08092315843328314,-0.304755401918555,-0.48891771226159314])\ntxt: String = 0.08092315843328314, -0.304755401918555, -0.48891771226159314\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 23
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val pheno_data = sc.textFile(phenotypes_path)\nval phenotypes = pheno_data.map(_.split('\\t')).map{\n  x =>\n  val terms = x(4).split(\";\")\n  x(0) -> terms\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "pheno_data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[170] at textFile at <console>:69\nphenotypes: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[172] at map at <console>:70\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[172] at map at &lt;console&gt;:70"
      },
      "output_type" : "execute_result",
      "execution_count" : 24
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val res = predicted_clusters.join(data).map{\n  case (patient, (cluster_id, vector)) =>\n  (cluster_id, vector.toArray.mkString(\", \") + \"\\n\")\n}.persist\n\nval nb_members = res.mapValues(x => 1).reduceByKey(_ + _)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[176] at map at <console>:77\nnb_members: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[178] at reduceByKey at <console>:82\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ShuffledRDD[178] at reduceByKey at &lt;console&gt;:82"
      },
      "output_type" : "execute_result",
      "execution_count" : 25
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val tot = predicted_clusters.join(effects).map{\n  case (patient, (cluster_id, effect_vector)) =>\n  (cluster_id, effect_vector)\n}.\nreduceByKey((a,b) =>  (a zip b).map{ case (a_val, b_val) => a_val + b_val }).\njoin(nb_members).mapValues{\n    case (array, nb_mem) =>\n    array.map( _ *1.0 / nb_mem)\n}\n\ntot.collect.foreach{\n  case (id, res) =>\n  txt = \"Proportions of each mutations over cluster \" + id + \" :\\n\" + \" HIGH: \" + res(0) + \"\\n MODERATE:\" + res(1)\n  txt += \"\\n LOW:\" + res(2) + \"\\n MODIFIER:\" + res(3) + \"\\n\"\n  //println(txt)\n  t.write(txt)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Proportions of each mutations over cluster 0 :\n HIGH: 32.0\n MODERATE:604.0\n LOW:26.0\n MODIFIER:32.0\n\nProportions of each mutations over cluster 1 :\n HIGH: 24.0\n MODERATE:440.0\n LOW:18.0\n MODIFIER:20.0\n\nProportions of each mutations over cluster 2 :\n HIGH: 19.0\n MODERATE:446.0\n LOW:28.0\n MODIFIER:22.0\n\ntot: org.apache.spark.rdd.RDD[(Int, Array[Double])] = MapPartitionsRDD[187] at mapValues at <console>:103\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 26
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val avg_vector = predicted_clusters.join(data).map{\n  case (patient, (cluster_id, vector)) =>\n  (cluster_id, vector.toArray)\n}.\nreduceByKey((a,b) =>  (a zip b).map{ case (a_val, b_val) => a_val + b_val }).\njoin(nb_members).mapValues{\n  case (vector, nb) =>\n  vector.map( _ / nb)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "avg_vector: org.apache.spark.rdd.RDD[(Int, Array[Double])] = MapPartitionsRDD[196] at mapValues at <console>:76\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[196] at mapValues at &lt;console&gt;:76"
      },
      "output_type" : "execute_result",
      "execution_count" : 27
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val phenos_by_cluster = predicted_clusters.join(phenotypes).values\n\nval counts_by_pheno = phenos_by_cluster.map{\n  case(cluster, pheno_list) => pheno_list.map(e => ((cluster, e), 1))\n}.flatMap{a => a.toList}.reduceByKey(_ + _).persist\n\nval counts_overall = counts_by_pheno.map{\n  case ((cluster, hpo), cnt) =>\n  (hpo, cnt)\n}.reduceByKey(_ + _)\n\nval counts_by_cluster_with_percentages = counts_by_pheno.map{\n  case ((cluster, hpo), cnt) =>\n  (hpo, (cluster, cnt))\n}.join(counts_overall).map{\n  case (hpo, ((cluster, cnt), tot)) =>\n  (cluster, (hpo, cnt, cnt*100.0/tot))\n}.groupByKey",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "phenos_by_cluster: org.apache.spark.rdd.RDD[(Int, Array[String])] = MapPartitionsRDD[200] at values at <console>:97\ncounts_by_pheno: org.apache.spark.rdd.RDD[((Int, String), Int)] = ShuffledRDD[203] at reduceByKey at <console>:101\ncounts_overall: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[205] at reduceByKey at <console>:106\ncounts_by_cluster_with_percentages: org.apache.spark.rdd.RDD[(Int, Iterable[(String, Int, Double)])] = ShuffledRDD[211] at groupByKey at <console>:114\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ShuffledRDD[211] at groupByKey at &lt;console&gt;:114"
      },
      "output_type" : "execute_result",
      "execution_count" : 28
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val patients_by_cluster = predicted_clusters.map{case (patient, cluster) => (cluster, patient)}.reduceByKey(_ + \", \" + _)\n\nres.aggregateByKey(\"\")(_ +_, _+_).\njoin(nb_members).join(patients_by_cluster).join(avg_vector).join(counts_by_cluster_with_percentages).\ncollect.foreach{\n  case (cluster_id, ((((ensemble, nb), patients_list), avg_vector), hpo_array)) =>\n  txt = \"----- CLUSTER \" + cluster_id + \" -----\\n\"\n  txt += \"Total of \" + nb + \" patients :\" + patients_list.toString + \"\\n\"\n  txt += \"Average vector: \" + avg_vector.mkString(\",\") + \"\\n\\n\"\n  txt += ensemble\n  //println(txt)\n  t.write(txt)\n  hpo_array.foreach{\n    case (hpo, cnt, perc) =>\n    val formatted_res = \"%-40s: %d (%.1f%% of the ppl having this HPO)\".format(hpo, cnt, perc)\n    //println(formatted_res)\n    t.write(formatted_res + \"\\n\")\n  }\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "patients_by_cluster: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[213] at reduceByKey at <console>:104\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 29
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "t.close()",
    "outputs" : [ {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 30
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ {
      "ename" : "Error",
      "output_type" : "error",
      "traceback" : [ "Incomplete (hint: check the parenthesis)" ]
    } ]
  } ],
  "nbformat" : 4
}