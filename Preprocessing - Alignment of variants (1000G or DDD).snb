{
  "metadata" : {
    "name" : "Preprocessing - Alignment of variants (1000G or DDD)",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#Preprocessing: Alignment of variants\n\nThe alignment is necessary for machine learning techniques that compare features vectors, such as K-Means.\n\nThe process is adjusted to the structure of variants annotated with [Highlander](http://sites.uclouvain.be/highlander/) and stored in [Parquet](https://parquet.apache.org/) files.\n\nIt was notably applied to variants from the [1000 Genomes data](http://1000genomes.org/) and [DDD cohort](https://decipher.sanger.ac.uk/ddd#overview)."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We define the parameters of the process:\n* the number of patients to consider\n* the reject list of patients we may not want to include\n* the path to the Parquet files containing variants\n* the HDFS address to save the output file to\n* the name of the output file"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val nb_patients = 3\nval reject_list = Array(\"\")\nval pathVariants = \"/user/hive/warehouse/1000g.db/exomes_1000g\" //or \"/user/hive/warehouse/1000g.db/ddd\"\nval hdfs_path = \"hdfs:/user/ndewit/\"\nval output_path = \"aligned_1000G_variants\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "nb_patients: Int = 3\nreject_list: Array[String] = Array(\"\")\npathVariants: String = /user/hive/warehouse/1000g.db/ddd\noutput_path: String = hdfs:/user/ndewit/\ndest: String = aligned_DDD_variants\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "aligned_DDD_variants"
      },
      "output_type" : "execute_result",
      "execution_count" : 19
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Retrieval from database\n\nWe will use SparkSQL to query the variants we need. The library is imported by default in Spark Notebook, but needs to be added to the application's dependencies if the code is transformed into a standalone application.\n\nWe read from Parquet files and make the equivalent of a relational database table to which SQL queries can be addressed. The flag \"binaryAsString\" is set explicitly to avoid compatibility problems with some Parquet-producing systems (Impala, Hive and older versions of SparkSQL do not differentiate between binary data and strings when writing out the Parquet schema)."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\nsqlContext.sql(\"SET spark.sql.parquet.binaryAsString=true\")\nimport sqlContext.implicits._\n\nval parquetFile = sqlContext.read.parquet(pathVariants)\nparquetFile.registerTempTable(\"variantData\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@6a6fb99\nimport sqlContext.implicits._\nparquetFile: org.apache.spark.sql.DataFrame = [id: int, platform: string, outsourcing: string, project_id: int, run_label: string, patient: string, pathology: string, partition: int, sample_type: string, chr: string, pos: int, reference: string, alternative: string, change_type: string, hgvs_protein: string, hgvs_dna: string, gene_symbol: string, exon_intron_rank: int, exon_intron_total: int, cdna_pos: int, cdna_length: int, cds_pos: int, cds_length: int, protein_pos: int, protein_length: int, gene_ensembl: string, num_genes: int, biotype: string, transcript_ensembl: string, transcript_uniprot_id: string, transcript_uniprot_acc: string, transcript_refseq_prot: string, tran..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 20
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "This step differs depending on the cohort used.\n\n1000G : We use that table to get a list of distinct patients IDs, limiting it to the number of patients we wish to work on. The *sql()* method of SQLContext allows to use SQL syntax to do so.\n\nDDD : We cannot take patients directly from the database, as we do not know which DDD patients are affected children and which are parent controls. To select appropriate IDs, we thus need to go through the file describing family relationships."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//For 1000G\nval patients_id = sqlContext.\nsql(\"SELECT DISTINCT patient FROM variantData \" +\n    \" LIMIT \" + nb_patients.toString).\nmap(_.getString(0)).collect\n\n//Alternatively, for DDD\n/*\nval base_path = \"datasets/ddd/\"\nval sourceFile = base_path + \"ddd3_family_relationships.txt\" // + cache if dataset big\nval source_data = sc.textFile(sourceFile)\nval families = source_data.map(_.split('\\t')).filter(_(2) != \"0\").map(x => (x(1), x(2), x(3)))\nval patients_id = families.map(_._1).take(nb_patients)\n*/",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "base_path: String = datasets/ddd/\nsourceFile: String = datasets/ddd/ddd3_family_relationships.txt\nsource_data: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[96] at textFile at <console>:86\nfamilies: org.apache.spark.rdd.RDD[(String, String, String)] = MapPartitionsRDD[99] at map at <console>:87\npatients_id: Array[String] = Array(DDDP101968, DDDP102189, DDDP111239)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"container-fluid\"><div><div class=\"col-md-12\"><div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonea38f51f2010d1b85c4e5f34287fa4f0&quot;,&quot;dataInit&quot;:[{&quot;string value&quot;:&quot;DDDP101968&quot;},{&quot;string value&quot;:&quot;DDDP102189&quot;},{&quot;string value&quot;:&quot;DDDP111239&quot;}],&quot;genId&quot;:&quot;1299844233&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"string value\"],\"nrow\":3,\"shown\":3,\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    </div></div></div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 21
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "/*\nval gene_list = List(\"datasets/ddd/ASD_genes\",\n                      \"datasets/ddd/DDD_genes\",\n                      \"datasets/ddd/ID_genes\").\nmap(path => sc.textFile(path)).\nreduce(_ union _).\ntoDF(\"gene_list\").\ndistinct\n*/",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We query variants for these patients based on the criteria of our choice: quality filters, allele number, presence in a given chromosome, minor allele frequency..\n\nWe will here get the content of the variants themselves, i.e. the individual alleles and their position, limiting ourselves to the 22nd chromosome.\n\nIf the list of patients is very long, we cannot just add them to the original SQL query, since a \"WHERE\" clause of hundreds of elements causes a StackOverflow error. We thus first apply our SQL query regardless of our patients selection, get a DataFrame in return, and only then use the *where()* method which filters elements based on the column(s) of our choice. We can then *map* our list of accepted patients to another column of literal values through the *lit()* function, and check the membership of elements from the first in the second via the *isin()* method.\nA similar technique is applied to filter out patients listed in the reject list.\n\nOnce the filtering is done, we can extract the content of the remaining lines. By default, the content of each column in a DataFrame is a String, but we can convert it to the type needed (this line is where the processing gets long since it is the point where the data is extracted in practice).\n\nFinally, we need to group the variants based on their associated patients to get an initial patients representation. A *groupByKey* function exists in Spark, but is known to create an overhead of shuffling operations. Instead, we use *aggregateByKey* which groups elements locally before merging the formed groups amongst remote partitions.\nBy default this operation returns a RDD of Iterables (containing the different variants as tuples of form (Position, Allele)), which we convert to an Array for more flexibility in the subsequent operations."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def make_request(cols : String) : String = {\n  var request = \"SELECT \" + cols + \" \"\n  request += \"FROM variantData \"\n  request += \"WHERE (\"\n  request += \"filters = 'PASS' \"\n  request += \"AND allele_num <= 2 \"\n  request += \"AND gene_symbol IS NOT NULL \"\n  //request += \"AND consensus_maf < 0.01 \"\n  request += \"AND chr = 22 \"\n  request += \")\"\n  return request\n}\n\nimport org.apache.spark.sql.functions.lit\n\nval initial_by_patient = sqlContext.\nsql(make_request(\"patient, pos, alternative\")).\nwhere($\"patient\".isin(patients_id.map(lit(_)):_*)).\nwhere(!$\"patient\".isin(reject_list.map(lit(_)):_*)).\n//join(gene_list, $\"gene_symbol\" === $\"gene_list\").\n//drop(\"gene_symbol\").\nmap{ row => (row.getString(0), (row.getInt(1), row.getString(2))) }.\ndistinct.\naggregateByKey(scala.collection.mutable.HashSet.empty[(Int, String)])(_+_, _++_).\nmapValues(_.toArray)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "make_request: (cols: String)String\nimport org.apache.spark.sql.functions.lit\ninitial_by_patient: org.apache.spark.rdd.RDD[(String, Array[(Int, String)])] = MapPartitionsRDD[110] at mapValues at <console>:110\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[110] at mapValues at &lt;console&gt;:110"
      },
      "output_type" : "execute_result",
      "execution_count" : 22
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Optionally, we can check the number of variants that were extracted for each patient. When using the standalone application, it is not a recommended operation since it needs to *collect* all lines to the driver, and the RDD will need to be recalculated afterwards unless it is explicitly cached."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/*initial_by_patient.mapValues(_.length).collect.foreach{\n  e =>\n  println(\"Nb of variants for patient \" + e._1 + \": \" + e._2) \n}*/",
    "outputs" : [ {
      "ename" : "Error",
      "output_type" : "error",
      "traceback" : [ "Incomplete (hint: check the parenthesis)" ]
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "As the positions present are not the same in every patient, we also separately query all possible positions that appear in our patients of interest. As we will need to associate that list with all patient rows, we turn it into a single RDD row and perform the cartesian product of it and the list of all patient IDs.\n\nEach element of the resulting RDD thus has both the patient's ID and the complete list of positions it should include."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val all_pos = sqlContext.\nsql(make_request(\"patient, pos\")).\nwhere($\"patient\".isin(patients_id.map(lit(_)):_*)).\nwhere(!$\"patient\".isin(reject_list.map(lit(_)):_*)).\nselect(\"pos\").distinct.\nmap(_.getInt(0)).\nmap((0, _)).groupByKey.values. //trick to group all variants into one RDD row\nmap(_.toArray)\n\nval all_pos_per_patient = sc.parallelize(patients_id).cartesian(all_pos)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "all_pos: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[123] at map at <console>:106\nall_pos_per_patient: org.apache.spark.rdd.RDD[(String, Array[Int])] = CartesianRDD[125] at cartesian at <console>:108\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "CartesianRDD[125] at cartesian at &lt;console&gt;:108"
      },
      "output_type" : "execute_result",
      "execution_count" : 24
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Alignment\n\nWe now produce the alignment in a couple of steps. We join the list of all variants to the list of variants each patient has, and for each of them:\n* We generate 1-of-k encoding for reference positions (which did not appear amongst the variants of the patient). *flatMap* is used to allow the mapping of 1 to 4 elements (from one position to four corresponding categories).\n* We generate 1-of-k encoding for the variants found in the variant, (only alternate unique nucleotides \"A\", \"C\", \"T\", \"G\" are considered, so point mutations exclusively ; CNVs are ignored in this method)\n* Note that it might be the case that multiple alleles correspond to a single position, which could could a problematic shift in the alignment. We thus make sure to only keep the first variant for the position.\n* We join both lists (mutated and reference variants) and order their content based on the variants positions. Once they are aligned, we can safely discard the positions and keep only the ordered features.\n* We convert the Array of features to a Vector to be used by machine learning techniques."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val samples = initial_by_patient.join(all_pos_per_patient).map{\n\n  case (patient, (features, all_pos)) =>\n\n  val pos = features.map(_._1) //list of positions present in the patient\n\n  val not_appearing = all_pos.filter(!pos.contains(_)). //for all positions not appearing in the patient\n  flatMap{\n    int_pos =>\n    val pos = BigDecimal(int_pos)\n    Array((pos + BigDecimal(0.1), 0.0),\n          (pos + BigDecimal(0.2), 0.0),\n          (pos + BigDecimal(0.3), 0.0),\n          (pos + BigDecimal(0.4), 0.0)\n         ).toList\n    //We add the four nucleotides as zeros\n  }\n\n  var features_four = features.flatMap{\n    case (int_pos, alt) =>\n    val pos = int_pos\n    var a = 0.0\n    var c = 0.0\n    var t = 0.0\n    var g = 0.0\n    if (alt == \"A\") { a = 1.0 }\n    else if (alt == \"C\") { c = 1.0 }\n    else if (alt == \"T\") { t = 1.0 }\n    else if (alt == \"G\") { g = 1.0 }\n\n    Array (\n      (pos + BigDecimal(0.1), a), \n      (pos + BigDecimal(0.2), c), \n      (pos + BigDecimal(0.3), t), \n      (pos + BigDecimal(0.4), g)\n    ).toList\n  }\n\n  //Manipulation to avoid multiple alleles for one position: group variants by common position,\n  //take only the first element of each resulting list (a rare but possible occurrece)\n  features_four = features_four.groupBy(_._1).map{ case (pos, list) => (pos, list.apply(0)._2)}.toArray\n\n  //Join both lists, order them by growing position and discard the positions to keep only the created features\n  val ordered_array = features_four.union(not_appearing).sortBy(_._1).map(_._2)\n\n  (patient, ordered_array)\n}.map{\n  case (patient, features) =>\n  (patient, org.apache.spark.mllib.linalg.Vectors.dense(features))\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "samples: org.apache.spark.rdd.RDD[(String, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[130] at map at <console>:141\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[130] at map at &lt;console&gt;:141"
      },
      "output_type" : "execute_result",
      "execution_count" : 25
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Similarly as before, we can check the length of the alignments by collecting them to the driver."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/*\n//Size = Alignment of all positions * 4 possible values for each\nsamples.mapValues(_.size).collect.foreach{\n  e =>\n  println(\"Size of alignment for patient \" + e._1 + \": \" + e._2) \n}\n*/",
    "outputs" : [ {
      "ename" : "Error",
      "output_type" : "error",
      "traceback" : [ "Incomplete (hint: check the parenthesis)" ]
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Save result\n\nFinally, we save our result as distributed files on the cluster. As the method fails if a file or folder with the same name exists, we first perform a recursive deletion of any conflicting element."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.hadoop.fs._\nimport java.net.URI\nval fs:FileSystem = FileSystem.get(new URI(output_path), sc.hadoopConfiguration)\nfs.delete(new Path(output_path), true) // \"True\" to activate recursive deletion of files if it is a folder\n\nsamples.saveAsObjectFile(output_path)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.hadoop.fs._\nimport java.net.URI\nfs: org.apache.hadoop.fs.FileSystem = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_668428907_9, ugi=ndewit (auth:SIMPLE)]]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 27
    } ]
  } ],
  "nbformat" : 4
}