{
  "metadata" : {
    "name" : "K-Means (Acute Inflammations)",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#Clustering: K-Means\n\nClustering applied to the [Acute inflammations dataset](https://archive.ics.uci.edu/ml/datasets/Acute+Inflammations) (UCI). As the dataset is not of large size, no separate preprocessing is necessary."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We define the parameters of the process:\n* the number of patients to consider\n* the path to the input file\n* the number of clusters to create\n* the number of iterations before considering the model has converged\n* the number of runs of the whole algorithm to compare results based on different stochastic initializations"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val nb_patients = 120\nval input_path = \"datasets/acute_inflammations/data\"\nval numClusters = 4\nval numIterations = 20\nval numRuns = 5",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Retrieval from database\n\nWe start by cleaning the data and making each row of the RDD into a feature vector. We thus convert from String to the desired types, and encode values of type \"yes/no\" into numerical booleans."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val raw_data = sc.parallelize(sc.textFile(input_path).take(nb_patients))\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nval elements = raw_data.zipWithIndex.map{\n  case (line, id) => \n  \n  val split = line.split('\\t')\n                            \n  val temperature = split(0).toDouble\n  val others = split.slice(1, 6).map{ v => if (v == \"no\") 0.0 else 1.0 }   \n  val res = Array(temperature) ++ others\n                            \n  val decisions = split.takeRight(2)\n  var label = 0\n  if (decisions(0) == \"no\" && decisions(1) == \"no\") {\n    label = 0 //\"neither\"\n  } else if (decisions(0) == \"yes\" && decisions(1) == \"no\") {\n    label = 1 //\"inflammation\"\n  } else if (decisions(0) == \"no\" && decisions(1) == \"yes\") {\n    label = 2 // \"nephretis\"\n  } else {\n    label = 3 //\"both\"\n  }\n  \n  ((id.toInt, label), Vectors.dense(res))\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We perform standardization of the features for more accurate machine learning processing."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.feature.StandardScaler\n\nval stdscaler = new StandardScaler(withMean = true, withStd = true).fit(elements.values)\n\nval samples = elements.mapValues{ stdscaler.transform(_)}.persist\n\nval nb_elements = samples.count.toInt",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Clustering process\n\nWe train a model based on our features."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n\nval kmeans_model = KMeans.train(samples.values, numClusters, numIterations, numRuns)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We evaluate the quality of the K-Means model by computing its Within Set Sum of Squared Errors"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val WSSSE =  kmeans_model.computeCost(samples.values)\nprintln(\"Within Set Sum of Squared Errors = \" + WSSSE)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Using the created model, we attribute a cluster to each input element."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val predicted_clusters = samples.mapValues{kmeans_model.predict(_)}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Visualization\n\nThis section makes uses of the WISP library, that allows to make Javascript graphs. We will use PCA to represent our samples in two dimensions and display them in a scatter plot."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n\nval nb_features_before = samples.values.first.size.toInt\nval matrix = new RowMatrix(samples.values)\n\nval K = 2 //Number of Principal Components to compute\nval pc = matrix.computePrincipalComponents(K) //returns a a org.apache.spark.mllib.linalg.Matrix\n\n//We map all elements to DenseMatrix in order to multiply() them\nval pc_mapped = new org.apache.spark.mllib.linalg.DenseMatrix(nb_features_before, K, pc.toArray)\nval mapped = samples.mapValues( v => new org.apache.spark.mllib.linalg.DenseMatrix(1, nb_features_before, v.toArray))\n\n//Project the element to the K-dimensional space spanned by the principal components\nval projected = mapped.mapValues{_.multiply(pc_mapped)}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import notebook.front.third.wisp._\nimport com.quantifind.charts.highcharts._\nimport com.quantifind.charts.highcharts.Highchart._",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def scatterplot_model(label_and_features: RDD[(Int, org.apache.spark.mllib.linalg.DenseMatrix)]) = {\n  val tmp = label_and_features.\n  mapValues( m =>  (m.apply(0,0), m.apply(0,1))).\n  groupByKey.values.\n  map(v => Pairs(v.toSeq, \"scatter\")).collect\n  \n  tmp.map(_.series).toSeq \n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "PlotH(Highchart(\n  scatterplot_model(projected.map{ case ((id, label), features) => (label, features) }),\n  title = \"True labeling (PCA to 2D)\",\n  xAxis = \"Dim1\",\n  yAxis = \"Dim2\"\n))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "PlotH(Highchart(\n  scatterplot_model(predicted_clusters.join(projected).values),\n  title = \"K-means labeling (PCA to 2D)\",\n  xAxis = \"Dim1\",\n  yAxis = \"Dim2\"\n))",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Clustering evaluation\n\nWe evaluate the quality of the clustering using the true labels of our samples. The evaluation index is called Rand Index, and its result is found between 0 and 1, with 1 being a perfect clustering.\n\nWe use Accumulators for *a* and *b* to allow these counts to be realized in parallel in the network and the result still returned to the driver."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def RID(to_eval : RDD[((Int, (Int, Int)), (Int, (Int, Int)))]) : Double = {\n  \n  def choose2(n : Int) : Double = {\n    return n * (n - 1) / 2;\n  }\n\n  val denom = choose2(nb_elements) //Denominator of RID is (nb_samples choose 2)\n  \n  // a : number of pairs in the same cluster in C and in K\n  // b : number of pairs in different clusters in C and in K\n  val a = sc.accumulator(0, \"Acc a : same cluster in both\")\n  val b = sc.accumulator(0, \"Acc b : different cluster in both\")\n\n  to_eval.foreach{\n    case ((id1, classes1), (id2, classes2)) =>\n\n    if (id1 != id2) {\n      if (classes1._1 == classes2._1 && classes1._2 == classes2._2) {\n        a += 1 //Classes match, and they should\n      }\n      else if (classes1._1 != classes2._1 && classes1._2 != classes2._2) {\n        b += 1 //Classes don't match, and they shouldn't\n      }\n    }\n  }\n\n  //We divide these counts by two since each pair was counted in both orders (a,b and b,a)\n  (a.value/2 + b.value/2) / denom\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We create tuples of the form (patient ID, true label, observed label) and run the external evaluation over them."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val both = predicted_clusters.map{ case ((id, label1), label2) => (id, (label1, label2))}\nval to_eval = both.cartesian(both)\nval eval_res = RID(to_eval)\n\nprintln(s\"RID = $eval_res | for nb_elements = $nb_elements & numClusters = $numClusters\")",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Plot a visualization of the misclassification of elements after the clustering\n\nNote: Relaunching this section multiple times may cause a \"Task not serializable\" error. Shut the notebook down and try again if that is the case."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//Separate true and false positive for each cluster\nval counts_attributions_by_true_label = predicted_clusters.map{\n  case ((id, label1), label2) =>\n  (label1, label2)\n}.groupByKey.\nmapValues{\n  labels =>\n  val counts = 0 until numClusters\n  counts.map( cluster_id => (cluster_id, labels.count( _ == cluster_id) ))\n}.sortBy(_._1)\n\nval model = counts_attributions_by_true_label.values.map{\n  x =>\n  Pairs(x, \"column\")\n}.collect.toSeq\n\nimport com.quantifind.charts.highcharts.Axis\n\nval names = (0 until numClusters).map{num => \"Cluster \" + num.toString}.toSeq\n\nPlot(\n  model,\n  xCat = Seq(names)\n)",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}