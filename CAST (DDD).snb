{
  "metadata" : {
    "name" : "CAST (DDD)",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#Clustering: CAST\n\nClustering applied to the [DDD cohort](https://decipher.sanger.ac.uk/ddd#overview). It is used after creation of a similarity matrix based on the elements to cluster (see the related notebook)."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We define the parameters of the process:\n* the path to our HDFS\n* the common path to the input files (similarity matrix S and set of elements U)\n* the path to the file containing the patients' phenotypes\n* the output path to print out results to HDFS\n\n* the affinity threshold"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val hdfs_path = \"hdfs:/user/ndewit/\"\nval input_path = hdfs_path + \"CASTUtils_DDD_metadata\"\nval phenotypes_path = hdfs_path + \"datasets/ddd/ddd3_ega-phenotypes.txt\"\nval output_path = hdfs_path + \"DDD_CAST_results.txt\"\n\nval threshold = 0.8",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Inputs retrieval\n\nWe retrieve both the similarity matrix S previously built and our set of elements U (containing name-id associations, without their features as we do not need them thanks to S).\n\nThe S matrix, which will be used on every executor node, can avoid being sent repeatedly thanks to Spark's broadcast variables, which instruct the executors to cache a read-only version of the matrix on each machine."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "var name_by_id = sc.objectFile[(Int, String)](input_path + \"_U\")\nval nb_elements = name_by_id.count.toInt\nval all_ids = name_by_id.keys.collect\n\nval S = sc.broadcast{\n  sc.objectFile[(Int, (Int, Double))](input_path + \"_S\").\n  aggregateByKey(scala.collection.mutable.HashSet.empty[(Int, Double)])(_+_, _++_).\n  mapValues(_.toMap[Int, Double]).collectAsMap\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Whenever an element is added or removed from a cluster, we update all affinities with regards to the cluster based on that element. The interest of having built the similarity matrix as (ID1, (ID2, value)) instead of ((ID1, ID2), value) is that we can easily select the whole row of values related to the added/removed element, since they always must all be updated at the same time."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def add_affinities(a: scala.collection.mutable.Map[Int, Double], S_row: Map[Int, Double]): Unit = {\n  for (element <- a) {\n    val id : Int = element._1\n    val new_affinity : Double = element._2 + S_row(id)\n    a -= id\n    a += (id -> new_affinity)\n  }\n}\n\ndef remove_affinities(a: scala.collection.mutable.Map[Int, Double], S_row: Map[Int, Double]): Unit = {\n  for (element <- a) {\n    val id : Int = element._1\n    val new_affinity : Double = element._2 - S_row(id)\n    a -= id\n    a += (id -> new_affinity)\n  }\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We define the core of the CAST algorithm.\n* We initialize a set containing all the elements belonging to already closed clusters in the form (Sample ID, Attributed cluster ID)\n* The original algorithm defines the set U as containing all elements ; however, since the comparison between elements is already contained in the similarity matrix, we can simply use the list of all element IDs instead (which lowers the memory pressure). Note that U will be modified during the clustering process, and can thus not be a RDD, since it would otherwise get shipped to executors where changes would occur, but these changes would never go back to the driver. We thus ground U into a non-distributed mutable set.\n* While elements remain in U, we alternate between Add and Remove operations\n* We keep track of the elements contained in the current cluster via the mutable set *C_open*\n* We keep track of the affinities of all elements with regards to the current cluster via the mutable Map *affinities*\n* We find the element that has maximum or minimum affinity wrt the cluster via the *maxBy()* function, linking it to the *affinities* Map.\n\nPrinted reports on the different actions can be uncommented to follow the algorithm's progress."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//C : Set containing the already closed clusters\nval C = scala.collection.mutable.Set.empty[(Int, Int)] //(Sample ID, Cluster)\nvar cluster_id = 0\n\n// We ground U into a non-RDD mutable Set\nval mut_U = scala.collection.mutable.Set.empty[Int] ++ all_ids\n\n// We do the whole thing while there are still elements in U\nwhile (!mut_U.isEmpty){\n\n  val remaining = mut_U.size\n  //println(s\"t=$threshold: $remaining elements remaining in U...\")\n\n  // Open a new current, open cluster of elements\n  var C_open = scala.collection.mutable.Set.empty[Int]\n\n  // Set all affinities wrt this new cluster to 0\n  var affinities = scala.collection.mutable.Map[Int, Double]() ++ all_ids.map(id => (id, 0.0)).toMap\n\n  var change_occurred : Boolean = false\n\n\n  do {\n\n    change_occurred = false\n\n\n    // ADD PHASE: Add high affinity elements to the current cluster\n\n    //println(\"-- ADD --\")\n\n    var max_element = -1\n    var max_affinity : Double = 0\n\n    // Avoid exceptions due to a max/reduce/maxBy on an empty collection\n    if (!mut_U.isEmpty) {\n\n      max_element = mut_U.maxBy(e => affinities(e))\n      max_affinity = affinities(max_element)\n      //println(s\"Max of U is $max_element with affinity $max_affinity\")\n    }\n\n    // While the last selected element is over the threshold\n    while ((!mut_U.isEmpty) && (max_affinity >= threshold*C_open.size)) {\n\n      //println(\"... and it is over threshold\")\n\n      val to_rem = mut_U.find(x => x == max_element).head\n      C_open += to_rem\n      mut_U -= to_rem\n\n      add_affinities(affinities, S.value.apply(max_element))\n\n      // We find the next maximal element\n      if (!mut_U.isEmpty) {\n        max_element = mut_U.maxBy(e => affinities(e))\n        max_affinity = affinities(max_element)\n        //println(s\"New max of U is $max_element with affinity $max_affinity\")\n      }\n\n      change_occurred = true\n    }\n\n\n    //-----------------------------------\n\n\n    // REMOVE PHASE : Remove low affinity elements to the current cluster\n\n    //println(\"-- REMOVE --\")\n\n    var min_element = -1\n    var min_affinity : Double = 0\n\n    if (!C_open.isEmpty) {\n\n      min_element = C_open.minBy(e => affinities(e))\n      min_affinity = affinities(min_element)\n\n      //println(s\"Min of C_open is $min_element with affinity $min_affinity\")\n    }\n\n    while (!C_open.isEmpty && min_affinity < threshold*C_open.size) {\n\n      //println(\"... and it is under threshold\")\n\n      val to_add = C_open.find(x => x == min_element).head\n      C_open -= to_add\n      mut_U += to_add\n\n      remove_affinities(affinities, S.value.apply(min_element))\n\n      //Find the next minimal element\n      if (!C_open.isEmpty) {\n        min_element = C_open.minBy(e => affinities(e))\n        min_affinity = affinities(min_element)\n        //println(s\"New min of U is $min_element with affinity $min_affinity\")\n      }\n\n      change_occurred = true\n    }\n\n\n  } while(change_occurred)\n\n  //println(s\"No changes occurred: this cluster (id $cluster_id) is complete.\\n\")\n\n  C_open.foreach{ e => C += ((e, cluster_id)) }\n  cluster_id = cluster_id + 1\n\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Clustering evaluation"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "##Output of results\n\nWe output our clustering results to a single file on HDFS. The file does not get written out until the *close()* method is reached."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.hadoop.fs._\nimport java.io.BufferedOutputStream\n\nval fs = FileSystem.get(sc.hadoopConfiguration)\n\nclass TextFile(file_path : String) {\n  val physical_file = fs.create(new Path(file_path))\n  val stream = new BufferedOutputStream(physical_file)\n\n  def write(text : String) : Unit = {\n    stream.write(text.getBytes(\"UTF-8\"))\n  }\n\n  def close() : Unit = {\n    stream.close()\n  }\n}\n\nval t = new TextFile(output_path)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We retrieve the list of phenotypes (HPO terms) for each patient."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val pheno_data = sc.textFile(phenotypes_path)\nval phenotypes = pheno_data.map(_.split('\\t')).map{\n  x =>\n  val terms = x(4).split(\";\")\n  x(0) -> terms\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We print all observations related to our clustering results:\n* for each cluster, we count the number of patients per population, and indicate the percentage of the people of this population it represents.\n* we count the average number of HPO terms per patient, averaged over the cluster\n* we list the HPO terms of each patient separately"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val predicted_clusters = sc.parallelize(C.toSeq)\nval cluster_by_name = name_by_id.join(predicted_clusters).values.persist\n\nval phenos_by_cluster = cluster_by_name.join(phenotypes).values\n\n//Listing all patients appearing in each cluster\nval patients_by_cluster = cluster_by_name.\nmap{case (patient, cluster) => (cluster, patient)}.reduceByKey(_ + \", \" + _)\n\n// We count the number of occurrences by phenotype, by cluster\nval counts_by_pheno = phenos_by_cluster.map{\n  case(cluster, pheno_list) => pheno_list.map(e => ((cluster, e), 1))\n}.flatMap{a => a.toList}.reduceByKey(_ + _).persist\n\n// We count the number of occurrences by phenotype over all clusters\nval counts_overall = counts_by_pheno.map{\n  case ((cluster, hpo), cnt) =>\n  (hpo, cnt)\n}.reduceByKey(_ + _)\n\n// We use the previous RDD to add the corresponding percentage of phenotypes to each occurrence count\nval counts_by_cluster_with_percentages = counts_by_pheno.map{\n  case ((cluster, hpo), cnt) =>\n  (hpo, (cluster, cnt))\n}.join(counts_overall).map{\n  case (hpo, ((cluster, cnt), tot)) =>\n  (cluster, (hpo, cnt, cnt*100.0/tot))\n}.groupByKey\n\nval nb_pheno_by_patient_by_cluster = cluster_by_name.join(phenotypes).map{\n  case (patient, (cluster, hpo_list)) =>\n  (cluster, (patient, hpo_list.size.toDouble, hpo_list))\n}.aggregateByKey(scala.collection.mutable.HashSet.empty[(String, Double, Array[String])])(_+_, _++_).\nmapValues(_.toArray)\n\nval avg_pheno_by_cluster = nb_pheno_by_patient_by_cluster.mapValues{\n  list =>\n  val sz_list = list.length\n  list.map(_._2).sum / sz_list\n}\n\ncounts_by_cluster_with_percentages.\njoin(patients_by_cluster).\njoin(nb_pheno_by_patient_by_cluster).\njoin(avg_pheno_by_cluster).\ncollect.foreach{\n  case (cluster, (((hpo_array, patients_list), patients_and_hpos), avg_nb_hpos)) =>\n  var txt = \"\\n\\n\\n--- Cluster \" + cluster.toString + \" ---\"\n  //println(txt)\n  t.write(txt + \"\\n\")\n\n  val sz = patients_list.split(\",\").size\n  val form1 = if (sz > 1) { \"s\" } else { \"\" }\n  val form2 = if (sz < 20) { \" : \" + patients_list.toString } else { \".\" }\n  txt = \"Total of \" + sz.toString + \" patient\" + form1 + form2\n  //println(txt)\n  t.write(txt + \"\\n\")\n\n  txt = \"Average nb of HPOs per patient: \" + avg_nb_hpos.toString\n  //println(txt)\n  t.write(txt + \"\\n\")\n\n  patients_and_hpos.foreach{\n    case (patient, nb_hpo, hpos) =>\n    txt = \"Patient \" + patient + \"has \" + nb_hpo.toInt.toString + \" HPO terms: \" + hpos.mkString(\", \")\n    //println(txt)\n    t.write(txt + \"\\n\")\n  }\n\n  hpo_array.toArray.sortWith(_._2 > _._2).foreach{\n    case (hpo, cnt, perc) =>\n    val form3 = if (cnt > 1) { \"people\" } else { \"person\" }\n    val formatted_res = \"%-40s: %d %s (%.1f%% of the ppl having this HPO)\".format(hpo, cnt, form3, perc)\n    //println(formatted_res)\n    t.write(formatted_res + \"\\n\")\n  }\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "t.close()",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}